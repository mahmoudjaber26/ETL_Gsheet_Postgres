name: Nightly Google Sheets → BigQuery ETL

on:
  schedule:
    - cron: "15 01 * * *"   # 01:15 UTC (≈ 04:15 Amman)
  workflow_dispatch:
    inputs:
      debug:
        description: "Enable verbose debug logging"
        required: false
        default: "false"

jobs:
  etl:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    env:
      BQ_DATASET: etl_dataset
      DEBUG: ${{ github.event.inputs.debug || 'false' }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          # removed cache to avoid requirements.txt/pyproject warning

      - name: Install deps
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip
          pip install \
            pandas==2.2.* \
            pyarrow>=14 \
            db-dtypes>=1.0.0\
            google-cloud-bigquery>=3.20.0 \
            gspread oauth2client
          pip freeze > .pip-freeze.txt
          

      - name: Sanity — files & Python
        run: |
          set -euxo pipefail
          ls -la
          python -c "import sys, pathlib, os; print('Python:', sys.version); print('Has config.json:', pathlib.Path('config.json').exists()); print('DEBUG:', os.getenv('DEBUG'))"

      - name: Write Google credentials file from secret
        run: |
          set -euxo pipefail
          echo "$GOOGLE_CREDENTIALS_JSON" > powerbi-etl-creds.json
          python -c "import json; d=json.load(open('powerbi-etl-creds.json')); print('Service account email:', d.get('client_email'))"
        env:
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}

      - name: Smoke — BigQuery permissions & dataset
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/powerbi-etl-creds.json
          BQ_DATASET: ${{ env.BQ_DATASET }}
        run: |
          set -euxo pipefail
          python - <<'PY'
          from google.cloud import bigquery
          from google.api_core.exceptions import NotFound
          import os, pandas as pd
          client = bigquery.Client()
          ds_id = f"{client.project}.{os.getenv('BQ_DATASET','etl_dataset').lower()}"
          try:
              client.get_dataset(ds_id)
              print("Dataset exists:", ds_id)
          except NotFound:
              print("Creating dataset:", ds_id)
              client.create_dataset(bigquery.Dataset(ds_id), exists_ok=True)
          tbl = f"{ds_id}._healthcheck_"
          df = pd.DataFrame([{"ok": 1}])
          job = client.load_table_from_dataframe(
              df, tbl,
              job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE", autodetect=True)
          )
          job.result()
          print("Healthcheck load OK")
          PY

      - name: Prepare log files (avoid missing-artifact errors)
        run: |
          :> logs.txt
          :> run-console.log

      - name: Run ETL (Sheets → BigQuery)
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/powerbi-etl-creds.json
          ETL_CONFIG_FILE: config.json
          BQ_DATASET: ${{ env.BQ_DATASET }}
          DEBUG: ${{ env.DEBUG }}
        run: |
          set -euxo pipefail
          python ETLGS_Postgres_gsheet_Final.py 2>&1 | tee -a run-console.log

      - name: Upload artifacts (logs & env)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: etl-run-artifacts
          path: |
            logs.txt
            run-console.log
            .pip-freeze.txt
            powerbi-etl-creds.json
          if-no-files-found: warn
